{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOrCarSnhY_d"
      },
      "source": [
        "# Module: Capstone Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDZZeAJYhY6m"
      },
      "source": [
        "## Section: Computer vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi9yeGtOhY0x"
      },
      "source": [
        "## <font color='#4073FF'> Project Solution: Garbage classification</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PJowKQahYuN"
      },
      "source": [
        "###  <font color='#14AAF5'>Classify garbage into one of the given classes</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM2LI7FAicld"
      },
      "source": [
        "### Project Brief:\n",
        "\n",
        "\n",
        "Waste segregation is included in law because it is much easier to recycle. Effective segregation of wastes means that less waste goes to landfill which makes it cheaper and better for people and the environment. It is also important to segregate for public health. In this project, you are required to segregate waste into one of the given categories using computer vision techniques. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZjJMst4icVY"
      },
      "source": [
        "### 1. Dataset\n",
        "\n",
        "The Garbage Classification Dataset contains 6 classifications: \n",
        "\n",
        "- cardboard (393) \n",
        "- glass (491)\n",
        "- metal (400)\n",
        "- paper(584)\n",
        "- plastic (472) \n",
        "- trash(127)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPno5CoHTZEm"
      },
      "outputs": [],
      "source": [
        "# Getting the contents of the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4RI_YIuTTVs"
      },
      "outputs": [],
      "source": [
        "#Importing libraries\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import optimizers, Input\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmdNMImUTTWD"
      },
      "source": [
        "### 2. Data collection and exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNjZu7UpTTWG",
        "outputId": "784a6c6c-01b4-4bc6-c911-9aa440a88c83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2285 images belonging to 6 classes.\n",
            "Found 252 images belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "# Defining an image data generator\n",
        "data = ImageDataGenerator(\n",
        "        preprocessing_function=tf.keras.applications.vgg16.preprocess_input,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip = True,\n",
        "        rescale=1./255, \n",
        "        validation_split=0.1,\n",
        "        )\n",
        "\n",
        "    \n",
        "# Define batch size\n",
        "batch_size=16 \n",
        "\n",
        "# Get pre-processed train images from directory\n",
        "traindata = data.flow_from_directory(directory=\"drive/MyDrive/Garbage classification\",\n",
        "                                     target_size = (224,224),\n",
        "                                     batch_size = batch_size,\n",
        "                                     class_mode = 'categorical',\n",
        "                                     subset  = 'training',\n",
        "                                     shuffle = True\n",
        "                                     )\n",
        "\n",
        "# Get pre-processed validation images from directory\n",
        "validationdata = data.flow_from_directory(directory=\"drive/MyDrive/Garbage classification\",\n",
        "                                     target_size = (224,224),\n",
        "                                     batch_size = batch_size,                                     \n",
        "                                     class_mode = 'categorical',\n",
        "                                     subset  = 'validation',\n",
        "                                     shuffle = True\n",
        "                                     )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Viewing class labels\n",
        "\n",
        "traindata.class_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dgBPjRozBuM",
        "outputId": "52c3d4f4-7086-4d37-ec42-a86d5f12c07b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cardboard': 0, 'glass': 1, 'metal': 2, 'paper': 3, 'plastic': 4, 'trash': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Data visualization"
      ],
      "metadata": {
        "id": "zWXytwgPWc_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class distribution\n",
        "# 3 Classes\n",
        "\n",
        "import seaborn as sns\n",
        "sns.countplot(x=traindata.classes)\n",
        "plt.title(\"Class Distribution\")\n",
        "plt.xlabel(\"Classes\")\n",
        "\n",
        "labels=['cardboard','glass','metal','paper','plastic','trash']\n",
        "\n",
        "plt.xticks(range(0,6), labels, rotation=55)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "Tw0a3WKE2Fpx",
        "outputId": "638819ff-f037-4468-ffee-4a278d85d9c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-lN2KeGlEGx"
      },
      "source": [
        "###4. Modelling\n",
        "\n",
        "#### **Methodology – Transfer Learning** \n",
        "\n",
        "Transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task.\n",
        "\n",
        "That means, taking the relevant parts of a pre-trained machine learning model and applying it to a new but similar problem. This will usually be the core information for the model to function, with new aspects added to the model to solve a specific task.The main benefits of transfer learning include the saving of resources and improved efficiency when training new models. It can also help with training models when only unlabelled datasets are available, as the bulk of the model will be pre-trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggo9HtqHTTWK"
      },
      "source": [
        "**CREATING THE MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2ymmfJgTTWL"
      },
      "source": [
        "Three different models were tested, all with 'imagenet' weights and fine-tuning (training only the new layers added)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsRYhSgBTTWL"
      },
      "outputs": [],
      "source": [
        "#Inception Resnet\n",
        "\n",
        "V = tf.keras.applications.InceptionResNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "\n",
        "# Setting trainable false\n",
        "for layer in V.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Adding some custom layers\n",
        "x = Conv2D(128, (3, 3), activation='relu')(V.output)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(128,activation='relu')(x)\n",
        "x = Dense(traindata.num_classes,activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=V.input, outputs=x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6ZrHd7RTTWP"
      },
      "source": [
        "\n",
        "**CHECKPOINTS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eJ8o9StTTWQ"
      },
      "outputs": [],
      "source": [
        "#checkpoints to check performance during training and save model weights/data.\n",
        "earlystopping = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=10)\n",
        "checkpointer = ModelCheckpoint(filepath=\"garbage_classification_weights.h5\", verbose=1, monitor='val_accuracy', mode='max', save_best_only=True, save_weights_only=False)\n",
        "csv_logger = CSVLogger(\"Log.csv\", append=True, separator=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lBp0JF5TTWR"
      },
      "source": [
        "**TRAINING THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iv6xOayETTWS"
      },
      "outputs": [],
      "source": [
        "#compiling the model\n",
        "opt = optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt,\n",
        "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "stepsTrain = (traindata.n // batch_size) \n",
        "stepsVal = (validationdata.n // batch_size) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFoxTnqATTWS",
        "outputId": "d13c3eb6-19e6-419b-aa65-8f8e4f2b8373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "142/142 [==============================] - ETA: 0s - loss: 1.5419 - accuracy: 0.6104\n",
            "Epoch 1: val_accuracy improved from -inf to 0.75000, saving model to garbage_classification_weights.h5\n",
            "142/142 [==============================] - 675s 5s/step - loss: 1.5419 - accuracy: 0.6104 - val_loss: 0.7424 - val_accuracy: 0.7500\n",
            "Epoch 2/50\n",
            "142/142 [==============================] - ETA: 0s - loss: 0.7622 - accuracy: 0.7250\n",
            "Epoch 2: val_accuracy did not improve from 0.75000\n",
            "142/142 [==============================] - 642s 5s/step - loss: 0.7622 - accuracy: 0.7250 - val_loss: 0.7308 - val_accuracy: 0.7208\n",
            "Epoch 3/50\n",
            "142/142 [==============================] - ETA: 0s - loss: 0.6958 - accuracy: 0.7488\n",
            "Epoch 3: val_accuracy did not improve from 0.75000\n",
            "142/142 [==============================] - 637s 4s/step - loss: 0.6958 - accuracy: 0.7488 - val_loss: 0.7193 - val_accuracy: 0.7500\n",
            "Epoch 4/50\n",
            "142/142 [==============================] - ETA: 0s - loss: 0.6381 - accuracy: 0.7770\n",
            "Epoch 4: val_accuracy did not improve from 0.75000\n",
            "142/142 [==============================] - 631s 4s/step - loss: 0.6381 - accuracy: 0.7770 - val_loss: 0.7138 - val_accuracy: 0.7417\n",
            "Epoch 5/50\n",
            "142/142 [==============================] - ETA: 0s - loss: 0.6025 - accuracy: 0.7823\n",
            "Epoch 5: val_accuracy improved from 0.75000 to 0.76667, saving model to garbage_classification_weights.h5\n",
            "142/142 [==============================] - 645s 5s/step - loss: 0.6025 - accuracy: 0.7823 - val_loss: 0.5906 - val_accuracy: 0.7667\n",
            "Epoch 6/50\n",
            "142/142 [==============================] - ETA: 0s - loss: 0.5523 - accuracy: 0.8052\n",
            "Epoch 6: val_accuracy improved from 0.76667 to 0.78750, saving model to garbage_classification_weights.h5\n",
            "142/142 [==============================] - 642s 5s/step - loss: 0.5523 - accuracy: 0.8052 - val_loss: 0.6349 - val_accuracy: 0.7875\n",
            "Epoch 7/50\n",
            "142/142 [==============================] - ETA: 0s - loss: 0.4826 - accuracy: 0.8286\n",
            "Epoch 7: val_accuracy did not improve from 0.78750\n",
            "142/142 [==============================] - 643s 5s/step - loss: 0.4826 - accuracy: 0.8286 - val_loss: 0.7830 - val_accuracy: 0.7625\n",
            "Epoch 8/50\n",
            "142/142 [==============================] - ETA: 0s - loss: 0.4532 - accuracy: 0.8409\n",
            "Epoch 8: val_accuracy did not improve from 0.78750\n",
            "142/142 [==============================] - 654s 5s/step - loss: 0.4532 - accuracy: 0.8409 - val_loss: 0.6947 - val_accuracy: 0.7583\n",
            "Epoch 9/50\n",
            "142/142 [==============================] - ETA: 0s - loss: 0.5216 - accuracy: 0.8056\n",
            "Epoch 9: val_accuracy did not improve from 0.78750\n",
            "142/142 [==============================] - 657s 5s/step - loss: 0.5216 - accuracy: 0.8056 - val_loss: 0.7047 - val_accuracy: 0.7667\n",
            "Epoch 10/50\n",
            "142/142 [==============================] - ETA: 0s - loss: 0.4665 - accuracy: 0.8330\n",
            "Epoch 10: val_accuracy did not improve from 0.78750\n",
            "142/142 [==============================] - 652s 5s/step - loss: 0.4665 - accuracy: 0.8330 - val_loss: 0.5911 - val_accuracy: 0.7875\n",
            "Epoch 11/50\n",
            "142/142 [==============================] - ETA: 0s - loss: 0.4512 - accuracy: 0.8365\n",
            "Epoch 11: val_accuracy did not improve from 0.78750\n",
            "142/142 [==============================] - 646s 5s/step - loss: 0.4512 - accuracy: 0.8365 - val_loss: 0.6611 - val_accuracy: 0.7750\n",
            "Epoch 12/50\n",
            "142/142 [==============================] - ETA: 0s - loss: 0.4397 - accuracy: 0.8378\n",
            "Epoch 12: val_accuracy did not improve from 0.78750\n",
            "142/142 [==============================] - 649s 5s/step - loss: 0.4397 - accuracy: 0.8378 - val_loss: 0.6871 - val_accuracy: 0.7792\n",
            "Epoch 13/50\n",
            "142/142 [==============================] - ETA: 0s - loss: 0.4108 - accuracy: 0.8528\n",
            "Epoch 13: val_accuracy did not improve from 0.78750\n",
            "142/142 [==============================] - 645s 5s/step - loss: 0.4108 - accuracy: 0.8528 - val_loss: 0.7338 - val_accuracy: 0.7500\n",
            "Epoch 14/50\n",
            "142/142 [==============================] - ETA: 0s - loss: 0.4029 - accuracy: 0.8594\n",
            "Epoch 14: val_accuracy did not improve from 0.78750\n",
            "142/142 [==============================] - 643s 5s/step - loss: 0.4029 - accuracy: 0.8594 - val_loss: 0.6805 - val_accuracy: 0.7750\n",
            "Epoch 15/50\n",
            "142/142 [==============================] - ETA: 0s - loss: 0.3868 - accuracy: 0.8528\n",
            "Epoch 15: val_accuracy did not improve from 0.78750\n",
            "142/142 [==============================] - 647s 5s/step - loss: 0.3868 - accuracy: 0.8528 - val_loss: 0.6818 - val_accuracy: 0.7333\n",
            "Epoch 16/50\n",
            "142/142 [==============================] - ETA: 0s - loss: 0.3484 - accuracy: 0.8770\n",
            "Epoch 16: val_accuracy did not improve from 0.78750\n",
            "142/142 [==============================] - 654s 5s/step - loss: 0.3484 - accuracy: 0.8770 - val_loss: 0.7318 - val_accuracy: 0.7833\n",
            "Epoch 16: early stopping\n",
            "INFO:tensorflow:Assets written to: vgg.model/assets\n"
          ]
        }
      ],
      "source": [
        "#model training for 50 epochs\n",
        "\n",
        "history = model.fit(traindata, epochs = 50, callbacks=[earlystopping, checkpointer, csv_logger], validation_data=validationdata,\n",
        "          steps_per_epoch=stepsTrain, validation_steps=stepsVal)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Save the model"
      ],
      "metadata": {
        "id": "b2SRsHXdWYeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model\n",
        "model.save('resv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se1amEWOcuUu",
        "outputId": "6128af0f-ea8e-44be-879c-0da7ebc64383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: resv/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading zip\n",
        "!zip -r resv.zip resv/"
      ],
      "metadata": {
        "id": "1JQMh1j6da83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7da05729-ce2d-4a20-809b-9c0b5409326a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: resv/ (stored 0%)\n",
            "  adding: resv/assets/ (stored 0%)\n",
            "  adding: resv/variables/ (stored 0%)\n",
            "  adding: resv/variables/variables.index (deflated 80%)\n",
            "  adding: resv/variables/variables.data-00000-of-00001 (deflated 9%)\n",
            "  adding: resv/saved_model.pb (deflated 92%)\n",
            "  adding: resv/keras_metadata.pb (deflated 96%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSG2V87RTTWU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load model\n",
        "model = tf.keras.models.load_model(\"resv\") \n",
        "\n",
        "# Get logs\n",
        "log = pd.read_csv(\"Log.csv\") "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "hTioqBqmdvBu",
        "outputId": "425c04e7-f947-4df4-ccb5-02bf79006813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ecc69034-90cc-42bf-8da7-1aa20eb762f3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.610401</td>\n",
              "      <td>1.541945</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.742394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.724989</td>\n",
              "      <td>0.762156</td>\n",
              "      <td>0.720833</td>\n",
              "      <td>0.730759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.748788</td>\n",
              "      <td>0.695804</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.719259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.776994</td>\n",
              "      <td>0.638075</td>\n",
              "      <td>0.741667</td>\n",
              "      <td>0.713800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.782283</td>\n",
              "      <td>0.602460</td>\n",
              "      <td>0.766667</td>\n",
              "      <td>0.590641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>0.805201</td>\n",
              "      <td>0.552254</td>\n",
              "      <td>0.787500</td>\n",
              "      <td>0.634911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>0.828559</td>\n",
              "      <td>0.482595</td>\n",
              "      <td>0.762500</td>\n",
              "      <td>0.782999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>0.840899</td>\n",
              "      <td>0.453225</td>\n",
              "      <td>0.758333</td>\n",
              "      <td>0.694750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>0.805641</td>\n",
              "      <td>0.521567</td>\n",
              "      <td>0.766667</td>\n",
              "      <td>0.704705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>0.832966</td>\n",
              "      <td>0.466489</td>\n",
              "      <td>0.787500</td>\n",
              "      <td>0.591131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>0.836492</td>\n",
              "      <td>0.451242</td>\n",
              "      <td>0.775000</td>\n",
              "      <td>0.661072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>0.837814</td>\n",
              "      <td>0.439747</td>\n",
              "      <td>0.779167</td>\n",
              "      <td>0.687116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>0.852799</td>\n",
              "      <td>0.410846</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.733820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>0.859409</td>\n",
              "      <td>0.402859</td>\n",
              "      <td>0.775000</td>\n",
              "      <td>0.680524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>0.852799</td>\n",
              "      <td>0.386752</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.681804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>0.877038</td>\n",
              "      <td>0.348370</td>\n",
              "      <td>0.783333</td>\n",
              "      <td>0.731780</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ecc69034-90cc-42bf-8da7-1aa20eb762f3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ecc69034-90cc-42bf-8da7-1aa20eb762f3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ecc69034-90cc-42bf-8da7-1aa20eb762f3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    epoch  accuracy      loss  val_accuracy  val_loss\n",
              "0       0  0.610401  1.541945      0.750000  0.742394\n",
              "1       1  0.724989  0.762156      0.720833  0.730759\n",
              "2       2  0.748788  0.695804      0.750000  0.719259\n",
              "3       3  0.776994  0.638075      0.741667  0.713800\n",
              "4       4  0.782283  0.602460      0.766667  0.590641\n",
              "5       5  0.805201  0.552254      0.787500  0.634911\n",
              "6       6  0.828559  0.482595      0.762500  0.782999\n",
              "7       7  0.840899  0.453225      0.758333  0.694750\n",
              "8       8  0.805641  0.521567      0.766667  0.704705\n",
              "9       9  0.832966  0.466489      0.787500  0.591131\n",
              "10     10  0.836492  0.451242      0.775000  0.661072\n",
              "11     11  0.837814  0.439747      0.779167  0.687116\n",
              "12     12  0.852799  0.410846      0.750000  0.733820\n",
              "13     13  0.859409  0.402859      0.775000  0.680524\n",
              "14     14  0.852799  0.386752      0.733333  0.681804\n",
              "15     15  0.877038  0.348370      0.783333  0.731780"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0ATZ9I6A2FK"
      },
      "source": [
        "### Model Evaluation on test images from google\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting class indices\n",
        "validationdata.class_indices.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKDebH6Rek2-",
        "outputId": "59c1cd6e-c8d5-44e6-ae4a-75e898833e3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash'])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "def predict_test(img_path):\n",
        "\n",
        "      #load the image\n",
        "      my_image = load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "      #preprocess the image\n",
        "      my_image = img_to_array(my_image)\n",
        "      my_image = my_image.reshape((1, my_image.shape[0], my_image.shape[1], my_image.shape[2]))\n",
        "      my_image = tf.keras.applications.vgg16.preprocess_input(my_image)\n",
        "      my_image = my_image/255.0\n",
        "\n",
        "      #make the prediction\n",
        "      labels = ['Cardboard', 'Glass', 'Metal', 'Paper', 'Plastic', 'Trash']\n",
        "      prediction = np.argmax(model.predict(my_image))\n",
        "      return labels[prediction]\n",
        "\n",
        "p1 = predict_test('drive/MyDrive/waste_test/t1.jpg')\n",
        "print(p1)\n",
        "\n",
        "p2 = predict_test('drive/MyDrive/waste_test/t2.jpg')\n",
        "print(p2)\n",
        "\n",
        "p3 = predict_test('drive/MyDrive/waste_test/t3.jpg')\n",
        "print(p3)\n",
        "\n",
        "p4 = predict_test('drive/MyDrive/waste_test/t4.jpg')\n",
        "print(p4)\n",
        "\n",
        "p5 = predict_test('drive/MyDrive/waste_test/t5.jpg')\n",
        "print(p5)\n",
        "\n",
        "p6 = predict_test('drive/MyDrive/waste_test/t6.jpg')\n",
        "print(p6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIAXeEMzELaI",
        "outputId": "7cef95b3-8393-4fd8-8b0e-c8ff93271b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Glass\n",
            "Cardboard\n",
            "Metal\n",
            "Paper\n",
            "Trash\n",
            "Trash\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "TDZZeAJYhY6m",
        "4PJowKQahYuN",
        "cM2LI7FAicld"
      ],
      "name": "Module15_Project_Capstone_Computer_vision.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}